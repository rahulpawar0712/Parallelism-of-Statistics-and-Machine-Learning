{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbors\n",
    "K-nearest neighbors is a non-parametric machine learning model in which the model\n",
    "memorizes the training observation for classifying the unseen test data. It can also be called\n",
    "instance-based learning. This model is often termed as lazy learning, as it does not learn\n",
    "anything during the training phase like regression, random forest, and so on. Instead it\n",
    "starts working only during the testing/evaluation phase to compare the given test\n",
    "observations with nearest training observations, which will take significant time in\n",
    "comparing each test data point. Hence, this technique is not efficient on big data; also,\n",
    "performance does deteriorate when the number of variables is high due to the curse of\n",
    "dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Bayes algorithm concept is quite old and exists from the 18th century since Thomas Bayes.\n",
    "Thomas developed the foundational mathematical principles for determining the\n",
    "probability of unknown events from the known events. For example, if all apples are red in\n",
    "color and average diameter would be about 4 inches then, if at random one fruit is selected\n",
    "from the basket with red color and diameter of 3.7 inch, what is the probability that the\n",
    "particular fruit would be an apple? Naive term does assume independence of particular\n",
    "features in a class with respect to others. In this case, there would be no dependency\n",
    "between color and diameter. This independece assumption makes the Naive Bayes\n",
    "classifier most effective in terms of computational ease for particular tasks such as email\n",
    "classification based on words in which high dimensions of vocab do exist, even after\n",
    "assuming independence between features. Naive Bayes classifier performs surprisingly\n",
    "really well in practical applications.\n",
    "\n",
    "Bayesian classifiers are best applied to problems in which information from a very high\n",
    "number of attributes should be considered simultaneously to estimate the probability of\n",
    "final outcome. Bayesian methods utilize all available evidence to consider for prediction\n",
    "even features have weak effects on the final outcome to predict. However, we should not\n",
    "ignore the fact that a large number of features with relatively minor effects, taken together\n",
    "its combined impact would form strong classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='SMSSpamCollection.txt' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsdata = open('SMSSpamCollection.txt','r')\n",
    "smsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader = csv.reader(smsdata,delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "smsdata_data = []\n",
    "smsdata_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in csv_reader:\n",
    "    smsdata_labels.append(line[0])\n",
    "    smsdata_data.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " 'Ok lar... Joking wif u oni...',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'U dun say so early hor... U c already then say...',\n",
       " \"Nah I don't think he goes to usf, he lives around here though\"]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsdata_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'ham', 'spam', 'ham', 'ham']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsdata_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "smsdata.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat... ham\n",
      "Ok lar... Joking wif u oni... ham\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's spam\n",
      "U dun say so early hor... U c already then say... ham\n",
      "Nah I don't think he goes to usf, he lives around here though ham\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print (smsdata_data[i],smsdata_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'ham': 4825, 'spam': 747})\n"
     ]
    }
   ],
   "source": [
    "# After getting preceding output run following code:\n",
    "from collections import Counter\n",
    "c = Counter( smsdata_labels )\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 5,572 observations, 4,825 are ham messages, which are about 86.5 percent and 747 spam messages are about remaining 13.4 percent.\n",
    "\n",
    "# Using NLP techniques, we have preprocessed the data for obtaining finalized word vectors to map with final outcomes spam or ham. Major preprocessing stages involved are:\n",
    "\n",
    "#### Removal of punctuations: Punctuations needs to be removed before applying any further processing. Punctuations from the string library are !\"# $%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~, which are removed from all the messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word tokenization: Words are chunked from sentences based on white space for further processing.\n",
    "\n",
    "\n",
    "#### Converting words into lower case: Converting to all lower case providesremoval of duplicates, such as Run and run, where the first one comes at start of the sentence and the later one comes in the middle of the sentence, and so on, which all needs to be unified to remove duplicates as we are working on bag of words technique.\n",
    "\n",
    "\n",
    "#### Stop word removal: Stop words are the words that repeat so many times in literature and yet are not much differentiator in explanatory power of sentences. For example: I, me, you, this, that, and so on, which needs to be removed before further processing.\n",
    "\n",
    "\n",
    "#### Keeping words of length at least three: Here we have removed words with length less than three. Stemming of words: Stemming process stems the words to its respective root words. Example of stemming is bringing down running to run or runs to run. By doing stemming we reduce duplicates and improve the accuracy of the model.\n",
    "\n",
    "\n",
    "#### Part-of-speech (POS) tagging: This applies the speech tags to words, such as noun, verb, adjective, and so on. For example, POS tagging for running is verb, whereas for run is noun. In some situation running is noun and lemmatization will not bring down the word to root word run, instead it just keeps the running as it is. Hence, POS tagging is a very crucial step necessary for performing prior to applying the lemmatization operation to bring down the word to its root word.\n",
    "\n",
    "\n",
    "#### Lemmatization of words: Lemmatization is another different process to reduce the dimensionality. In lemmatization process, it brings down the word to root word rather than just truncating the words. For example, bring ate to its root word as eat when we pass the ate word into lemmatizer with the POS tag as verb.\n",
    "\n",
    "## The nltk package has been utilized for all the preprocessing steps, as it consists of all the necessary NLP functionality in one single roof:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())   # 1\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]    # 2\n",
    "    tokens = [word.lower() for word in tokens]                                                    # 3\n",
    "    stopwds = stopwords.words('english')                                                          # 4\n",
    "    tokens = [token for token in tokens if token not in stopwds]                                  # 4\n",
    "    tokens = [word for word in tokens if len(word)>=3]                                            # 5\n",
    "    stemmer = PorterStemmer()                                                                     # 6\n",
    "    tokens = [stemmer.stem(word) for word in tokens]                                              # 6\n",
    "    tagged_corpus = pos_tag(tokens)                                                               # 7\n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    pre_proc_text=\" \".join([prat_lemmatize(token,tag) for token, tag in tagged_corpus])\n",
    "    return pre_proc_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The following line of the code splits the word and checks each character if it is in standard punctuations, if so it will be replaced with blank and or else it just does not replace with blanks # 1.\n",
    "2. The following code tokenizes the sentences into words based on white spaces and put them together as a list for applying further steps # 2\n",
    "3. Converting all the cases (upper, lower, and proper) into lowercase reduces duplicates in corpus: # 3\n",
    "4. As mentioned earlier, stop words are the words that do not carry much weight in understanding the sentence; they are used for connecting words, and so on. We have removed them with the following line of code # 4\n",
    "5. Keeping only the words with length greater than 3 in the following code for removing small words, which hardly consists of much of a meaning to carry # 5\n",
    "6. Stemming is applied on the words using PorterStemmer function, which stems the extra suffixes from the words # 6\n",
    "7. POS tagging is a prerequisite for lemmatization, based on whether the word is noun or verb, and so on, it will reduce it to the root word: # 7\n",
    "8. The pos_tag function returns the part of speed in four formats for noun and six formats for verb. NN (noun, common, singular), NNP (noun, proper, singular), NNPS (noun, proper, plural), NNS (noun, common, plural), VB (verb, base form), VBD (verb, past tense), VBG (verb, present participle), VBN (verb, past participle), VBP (verb, present tense, not third person singular), VBZ (verb, present tense, third person singular):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prat_lemmatize function has been created only for the reasons of mismatch between\n",
    "the pos_tag function and intake values of the lemmatize function. If the tag for any word\n",
    "falls under the respective noun or verb tags category, n or v will be applied accordingly in\n",
    "the lemmatize function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "smsdata_data_2 = []\n",
    "for i in smsdata_data:\n",
    "    smsdata_data_2.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set size for this classifier is 3900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "trainset_size = int(round(len(smsdata_data_2)*0.70))\n",
    "print ('The training set size for this classifier is ' + str(trainset_size) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([''.join(rec) for rec in smsdata_data_2[0:trainset_size]])\n",
    "y_train = np.array([rec for rec in smsdata_labels[0:trainset_size]])\n",
    "x_test = np.array([''.join(rec) for rec in smsdata_data_2[trainset_size+1:len( smsdata_data_2)]])\n",
    "y_test = np.array([rec for rec in smsdata_labels[trainset_size+1:len(smsdata_labels)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900,) (3900,) (1671,) (1671,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code converts the words into a vectorizer format and applies term\n",
    "frequency-inverse document frequency (TF-IDF) weights, which is a way to increase\n",
    "weights to words with high frequency and at the same time penalize the general terms such\n",
    "as the, him, at, and so on. In the following code, we have restricted to most frequent 4,000\n",
    "words in the vocabulary, none the less we can tune this parameter as well for checking\n",
    "where the better accuracies are obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building TFIDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),stop_words='english',\n",
    "                             max_features= 4000,strip_accents='unicode', norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = vectorizer.fit_transform(x_train).todense()\n",
    "x_test_2 = vectorizer.transform(x_test).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes classifier is suitable for classification with discrete features\n",
    "(example word counts), which normally requires large feature counts. However, in practice,\n",
    "fractional counts such as TF-IDF will also work well. If we do not mention any Laplace\n",
    "estimator, it does take the value of 1.0 means and it will add 1.0 against each term in\n",
    "numerator and total for denominator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(x_train_2, y_train)\n",
    "ytrain_nb_predicted = clf.predict(x_train_2)\n",
    "ytest_nb_predicted = clf.predict(x_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes - Train Confusion Matrix\n",
      "\n",
      " Predicted   ham  spam\n",
      "Actuall              \n",
      "ham        3381     0\n",
      "spam         77   442\n",
      "\n",
      "Naive Bayes- Train accuracy 0.98\n",
      "\n",
      "Naive Bayes - Train Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      3381\n",
      "        spam       1.00      0.85      0.92       519\n",
      "\n",
      "    accuracy                           0.98      3900\n",
      "   macro avg       0.99      0.93      0.95      3900\n",
      "weighted avg       0.98      0.98      0.98      3900\n",
      "\n",
      "\n",
      "Naive Bayes - Test Confusion Matrix\n",
      "\n",
      " Predicted   ham  spam\n",
      "Actuall              \n",
      "ham        1440     3\n",
      "spam         54   174\n",
      "\n",
      "Naive Bayes- Test accuracy 0.966\n",
      "\n",
      "Naive Bayes - Test Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1443\n",
      "        spam       0.98      0.76      0.86       228\n",
      "\n",
      "    accuracy                           0.97      1671\n",
      "   macro avg       0.97      0.88      0.92      1671\n",
      "weighted avg       0.97      0.97      0.96      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nNaive Bayes - Train Confusion Matrix\\n\\n\",\n",
    "       pd.crosstab(y_train, ytrain_nb_predicted,rownames =[\"Actuall\"],colnames = [\"Predicted\"]))\n",
    "print (\"\\nNaive Bayes- Train accuracy\",round(accuracy_score(y_train,ytrain_nb_predicted),3))\n",
    "print (\"\\nNaive Bayes - Train Classification Report\\n\",classification_report(y_train, ytrain_nb_predicted))\n",
    "print (\"\\nNaive Bayes - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,ytest_nb_predicted,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))\n",
    "print (\"\\nNaive Bayes- Test accuracy\",round(accuracy_score(y_test,ytest_nb_predicted),3))\n",
    "print (\"\\nNaive Bayes - Test Classification Report\\n\",classification_report( y_test, ytest_nb_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous results it is appearing that Naive Bayes has produced excellent results of\n",
    "96.6 percent test accuracy with significant recall value of 76 percent for spam and almost\n",
    "100 percent for ham.\n",
    "\n",
    "\n",
    "However, if we would like to check what are the top 10 features based on their coefficients\n",
    "from Naive Bayes, the following code will be handy for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 10 features - both first & last\n",
      "\n",
      "\t-8.7130\t1hr            \t\t-5.5795\tfree           \n",
      "\t-8.7130\t1st love       \t\t-5.7187\ttxt            \n",
      "\t-8.7130\t2go            \t\t-5.8721\ttext           \n",
      "\t-8.7130\t2morrow        \t\t-6.0066\tclaim          \n",
      "\t-8.7130\t2mrw           \t\t-6.0704\tstop           \n",
      "\t-8.7130\t2nd inning     \t\t-6.0785\tmobil          \n",
      "\t-8.7130\t2nd sm         \t\t-6.1074\trepli          \n",
      "\t-8.7130\t30ish          \t\t-6.1514\tprize          \n",
      "\t-8.7130\t3rd            \t\t-6.2015\tservic         \n",
      "\t-8.7130\t3rd natur      \t\t-6.2208\ttone           \n"
     ]
    }
   ],
   "source": [
    "# printing top features\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "coefs = clf.coef_\n",
    "intercept = clf.intercept_\n",
    "coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "print (\"\\n\\nTop 10 features - both first & last\\n\")\n",
    "n=10\n",
    "top_n_coefs = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "for (coef_1, fn_1), (coef_2, fn_2) in top_n_coefs:\n",
    "    print('\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s' % (coef_1, fn_1, coef_2,fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
